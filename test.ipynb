{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "015998d2-975b-4cb2-b991-082cee7e57ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n",
      "'unzip' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting gensim\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8a/6f/a690547cb7089d4019465bfbfbbb8bea5b3e52969cd2d6005049e6678ec4/gensim-4.2.0-cp37-cp37m-win_amd64.whl (24.0 MB)\n",
      "     -------------------------------------- 24.0/24.0 MB 395.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\environment\\anaconda3\\envs\\mindspore\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/79/311cfbca90332ab37ef8ea08f1af3266f20a9a0e7a1d652842db832226bb/Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "     ------------------------------------ 983.8/983.8 kB 356.0 kB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/07/36678c6ff0dfa6cf445d0e00bf4f013de3b86ec1a2e8bfd1e5df69b2d91d/smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "     -------------------------------------- 58.6/58.6 kB 443.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\environment\\anaconda3\\envs\\mindspore\\lib\\site-packages (from gensim) (1.21.6)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.2.0\n"
     ]
    }
   ],
   "source": [
    "# 下载数据集\n",
    "!wget https://ascend-professional-construction-dataset.obs.myhuaweicloud.com/deep-learning/aclImdb_v1.tar.gz\n",
    "# 下载模型\n",
    "!wget https://ascend-professional-construction-dataset.obs.myhuaweicloud.com/deep-learning/glove.zip\n",
    "!unzip glove.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe296727",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ascend-professional-construction-dataset.obs.myhuaweicloud.com/deep-learning/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de64aa-ee1b-495f-b32a-41778b5b6127",
   "metadata": {},
   "source": [
    "尝试改变每层神经元数量，但是发现过拟合明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da43690-25a1-4f28-b599-60e45b98b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##################依赖库导入######################\n",
    "#################################################\n",
    "import os\n",
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"依赖库导入成功\")\n",
    "#################################################\n",
    "##################参数配置########################\n",
    "#################################################\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "epochs_list = [10,20,50]\n",
    "batch_size_list = [256,512,1024]\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0\n",
    "\n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "max_pool_2 = layers.MaxPooling2D((4,4))\n",
    "max_pool_3 = layers.MaxPooling2D((8,8))\n",
    "#max_pool_list = [max_pool_1, max_pool_2]\n",
    "max_pool_list = [(2,2)]\n",
    "flat_layer = layers.Flatten()\n",
    "fc = layers.Dense(128, activation='relu')\n",
    "output = layers.Dense(10, 'softmax')\n",
    "\n",
    "drop_1 = keras.layers.Dropout(0.2)\n",
    "drop_2 = keras.layers.Dropout(0.2)\n",
    "drop_3 = keras.layers.Dropout(0.2)\n",
    "drop_4 = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv1_new = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02),input_shape=(28,28,1) )\n",
    "BN=keras.layers.BatchNormalization()\n",
    "conv1_new_conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv1_new_conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv1_new_conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "conv1_new_max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_2 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_3 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_flat_layer = layers.Flatten()\n",
    "conv1_new_fc = layers.Dense(128, activation='relu')\n",
    "conv1_new_output = layers.Dense(10, 'softmax')\n",
    "\n",
    "print(\"参数配置成功\")\n",
    "#################################################\n",
    "##################训练开始########################\n",
    "#################################################\n",
    "# TensorFlow Keras 使用Keras Sequential API\n",
    "for i in range(len(epochs_list)):\n",
    "    for j in range(len(batch_size_list)):\n",
    "        for k in range(len(max_pool_list)):\n",
    "            print(\"##########################################################################\")\n",
    "            print(\"开始训练模型，epochs为\"+str(epochs_list[i])+\", batch_size为\"+str(batch_size_list[j])+\", max_pool为\"+str(max_pool_list[k]))\n",
    "            model = models.Sequential()\n",
    "            #if k == 1:\n",
    "            #    model.summary()\n",
    "            model.add(conv1)\n",
    "            model.add(conv2)\n",
    "            model.add(conv3)\n",
    "            model.add(layers.MaxPooling2D(max_pool_list[k]))\n",
    "            model.add(flat_layer)\n",
    "            model.add(fc)\n",
    "            model.add(output)\n",
    "\n",
    "            model.summary()\n",
    "            model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            ################################################\n",
    "            model.fit(train_images_norm, train_labels, epochs=epochs_list[i], batch_size=batch_size_list[j], shuffle=True, validation_split=0.1)\n",
    "            ################################################\n",
    "            test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            \n",
    "            new_model = models.Sequential()\n",
    "\n",
    "            new_model.add(conv1_new_conv1)\n",
    "            #第一卷积层'conv1'没有池化层和Dropout层\n",
    "            new_model.add(conv1_new_conv2)\n",
    "            new_model.add(conv1_new_max_pool_2)\n",
    "            new_model.add(drop_2)\n",
    "            new_model.add(conv1_new_conv3)\n",
    "            new_model.add(conv1_new_max_pool_3)\n",
    "            new_model.add(drop_3)\n",
    "            new_model.add(conv1_new_flat_layer)\n",
    "            new_model.add(conv1_new_fc)\n",
    "            new_model.add(BN)\n",
    "            new_model.add(drop_4)\n",
    "            new_model.add(conv1_new_output)\n",
    "\n",
    "            new_model.summary()\n",
    "            new_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            new_model.fit(train_images_norm, train_labels, epochs=30, batch_size=512, shuffle=True, validation_split=0.1)\n",
    "            test_loss, test_accuracy = new_model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            print(test_loss)\n",
    "            print(new_model.history.history.keys())\n",
    "            #plt.subplot(3*3*1,3,3*i+j+1)\n",
    "            plt.figure(3*i+j+1)\n",
    "            plt.plot(new_model.history.history['accuracy'])\n",
    "            plt.plot(new_model.history.history['val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            #plt.subplot(3*3*1,3,3*i+j+1+1)\n",
    "            plt.figure(3*i+j+2)\n",
    "            plt.plot(new_model.history.history['loss'])\n",
    "            plt.plot(new_model.history.history['val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            ############模型预测#################\n",
    "#             predicted_test_labels = new_model.predict(test_images_norm)\n",
    "#             print(predicted_test_labels.shape)\n",
    "#             print(predicted_test_labels[88])\n",
    "\n",
    "\n",
    "#             predicted_test_labels_index = np.argmax(predicted_test_labels[88])\n",
    "\n",
    "#             print(predicted_test_labels_index)\n",
    "#             print(test_labels[88])\n",
    "\n",
    "#             #plt.subplot(3*3*1,3,3*i+j+2+1)\n",
    "#             plt.figure(3*i+j+3)\n",
    "#             plt.figure()\n",
    "#             plt.imshow(np.squeeze(test_images[88]))\n",
    "#             plt.show()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for key in range(5):\n",
    "                number = random.randint(0,10000)\n",
    "                print(\"number is \"+str(number))\n",
    "                plt.subplot(1,5,key+1)\n",
    "                predicted_test_labels = new_model.predict(test_images_norm)\n",
    "                print(predicted_test_labels.shape)\n",
    "                print(predicted_test_labels[number])\n",
    "\n",
    "                predicted_test_labels_index = np.argmax(predicted_test_labels[number])\n",
    "\n",
    "                print(predicted_test_labels_index)\n",
    "                print(test_labels[number])\n",
    "\n",
    "                plt.imshow(np.squeeze(test_images[number]))\n",
    "            plt.show()\n",
    "print(\"模型不同参数配置成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688cd8f-035c-43e0-8234-d9059a2602b4",
   "metadata": {},
   "source": [
    "发现轮回次数如果从20轮开始，在已经测试的batchsize下过拟合现象很明显，所以接下来测试一下降低epoch以及batchsize的效果。但是从测试集里面随机取5个进行识别分类发现准确率也还行，可能是“5”这个样本量太小了，偶然性太大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a102d-2fd6-4b93-b829-fb23672fb533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##################依赖库导入######################\n",
    "#################################################\n",
    "import os\n",
    "import struct\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"依赖库导入成功\")\n",
    "#################################################\n",
    "##################参数配置########################\n",
    "#################################################\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "epochs_list = [3,5,10]\n",
    "batch_size_list = [128,256,512]\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0\n",
    "\n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "max_pool_2 = layers.MaxPooling2D((4,4))\n",
    "max_pool_3 = layers.MaxPooling2D((8,8))\n",
    "#max_pool_list = [max_pool_1, max_pool_2]\n",
    "max_pool_list = [(2,2)]\n",
    "flat_layer = layers.Flatten()\n",
    "fc = layers.Dense(128, activation='relu')\n",
    "output = layers.Dense(10, 'softmax')\n",
    "\n",
    "drop_1 = keras.layers.Dropout(0.2)\n",
    "drop_2 = keras.layers.Dropout(0.2)\n",
    "drop_3 = keras.layers.Dropout(0.2)\n",
    "drop_4 = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv1_new = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02),input_shape=(28,28,1) )\n",
    "BN=keras.layers.BatchNormalization()\n",
    "conv1_new_conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv1_new_conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv1_new_conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "conv1_new_max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_2 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_3 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_flat_layer = layers.Flatten()\n",
    "conv1_new_fc = layers.Dense(128, activation='relu')\n",
    "conv1_new_output = layers.Dense(10, 'softmax')\n",
    "\n",
    "print(\"参数配置成功\")\n",
    "#################################################\n",
    "##################训练开始########################\n",
    "#################################################\n",
    "# TensorFlow Keras 使用Keras Sequential API\n",
    "for i in range(len(epochs_list)):\n",
    "    for j in range(len(batch_size_list)):\n",
    "        for k in range(len(max_pool_list)):\n",
    "            print(\"##########################################################################\")\n",
    "            print(\"开始训练模型，epochs为\"+str(epochs_list[i])+\", batch_size为\"+str(batch_size_list[j])+\", max_pool为\"+str(max_pool_list[k]))\n",
    "            model = models.Sequential()\n",
    "            #if k == 1:\n",
    "            #    model.summary()\n",
    "            model.add(conv1)\n",
    "            model.add(conv2)\n",
    "            model.add(conv3)\n",
    "            model.add(layers.MaxPooling2D(max_pool_list[k]))\n",
    "            model.add(flat_layer)\n",
    "            model.add(fc)\n",
    "            model.add(output)\n",
    "\n",
    "            model.summary()\n",
    "            model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            ################################################\n",
    "            model.fit(train_images_norm, train_labels, epochs=epochs_list[i], batch_size=batch_size_list[j], shuffle=True, validation_split=0.1)\n",
    "            ################################################\n",
    "            test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            \n",
    "            new_model = models.Sequential()\n",
    "\n",
    "            new_model.add(conv1_new_conv1)\n",
    "            #第一卷积层'conv1'没有池化层和Dropout层\n",
    "            new_model.add(conv1_new_conv2)\n",
    "            new_model.add(conv1_new_max_pool_2)\n",
    "            new_model.add(drop_2)\n",
    "            new_model.add(conv1_new_conv3)\n",
    "            new_model.add(conv1_new_max_pool_3)\n",
    "            new_model.add(drop_3)\n",
    "            new_model.add(conv1_new_flat_layer)\n",
    "            new_model.add(conv1_new_fc)\n",
    "            new_model.add(BN)\n",
    "            new_model.add(drop_4)\n",
    "            new_model.add(conv1_new_output)\n",
    "\n",
    "            new_model.summary()\n",
    "            new_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            new_model.fit(train_images_norm, train_labels, epochs=30, batch_size=512, shuffle=True, validation_split=0.1)\n",
    "            test_loss, test_accuracy = new_model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            print(test_loss)\n",
    "            print(new_model.history.history.keys())\n",
    "            #plt.subplot(3*3*1,3,3*i+j+1)\n",
    "            plt.figure(3*i+j+1)\n",
    "            plt.plot(new_model.history.history['accuracy'])\n",
    "            plt.plot(new_model.history.history['val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            #plt.subplot(3*3*1,3,3*i+j+1+1)\n",
    "            plt.figure(3*i+j+2)\n",
    "            plt.plot(new_model.history.history['loss'])\n",
    "            plt.plot(new_model.history.history['val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            ############模型预测#################\n",
    "#             predicted_test_labels = new_model.predict(test_images_norm)\n",
    "#             print(predicted_test_labels.shape)\n",
    "#             print(predicted_test_labels[88])\n",
    "\n",
    "\n",
    "#             predicted_test_labels_index = np.argmax(predicted_test_labels[88])\n",
    "\n",
    "#             print(predicted_test_labels_index)\n",
    "#             print(test_labels[88])\n",
    "\n",
    "#             #plt.subplot(3*3*1,3,3*i+j+2+1)\n",
    "#             plt.figure(3*i+j+3)\n",
    "#             plt.figure()\n",
    "#             plt.imshow(np.squeeze(test_images[88]))\n",
    "#             plt.show()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for key in range(5):\n",
    "                number = random.randint(0,10000)\n",
    "                print(\"number is \"+str(number))\n",
    "                plt.subplot(1,5,key+1)\n",
    "                predicted_test_labels = new_model.predict(test_images_norm)\n",
    "                print(predicted_test_labels.shape)\n",
    "                print(predicted_test_labels[number])\n",
    "\n",
    "                predicted_test_labels_index = np.argmax(predicted_test_labels[number])\n",
    "\n",
    "                print(predicted_test_labels_index)\n",
    "                print(test_labels[number])\n",
    "\n",
    "                plt.imshow(np.squeeze(test_images[number]))\n",
    "            plt.show()\n",
    "print(\"模型不同参数配置成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c7eea4-21fc-4f42-bcc4-ad5ade0bbe7e",
   "metadata": {},
   "source": [
    "从5轮开始已经是一直过拟合了，10轮的测试没有意义，直接停止测试。发现3轮，batchsize设置128的效果还行，在此基础上重新开始修改神经元数量的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b542d-5adf-47d8-8af8-13418ca99ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##################依赖库导入######################\n",
    "#################################################\n",
    "import os\n",
    "import struct\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"依赖库导入成功\")\n",
    "#################################################\n",
    "##################参数配置########################\n",
    "#################################################\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "epochs_list = [3]\n",
    "batch_size_list = [128]\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0\n",
    "conv_list = [[32,64,128],[64,64,128],[32,32,128]]\n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "max_pool_2 = layers.MaxPooling2D((4,4))\n",
    "max_pool_3 = layers.MaxPooling2D((8,8))\n",
    "max_pool_list = [max_pool_1, max_pool_3, max_pool_3]\n",
    "flat_layer = layers.Flatten()\n",
    "fc = layers.Dense(128, activation='relu')\n",
    "output = layers.Dense(10, 'softmax')\n",
    "\n",
    "drop_1 = keras.layers.Dropout(0.2)\n",
    "drop_2 = keras.layers.Dropout(0.2)\n",
    "drop_3 = keras.layers.Dropout(0.2)\n",
    "drop_4 = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv1_new = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02),input_shape=(28,28,1) )\n",
    "BN=keras.layers.BatchNormalization()\n",
    "conv1_new_conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv1_new_conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv1_new_conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "conv1_new_max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_2 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_3 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_flat_layer = layers.Flatten()\n",
    "conv1_new_fc = layers.Dense(128, activation='relu')\n",
    "conv1_new_output = layers.Dense(10, 'softmax')\n",
    "\n",
    "print(\"参数配置成功\")\n",
    "#################################################\n",
    "##################训练开始########################\n",
    "#################################################\n",
    "# TensorFlow Keras 使用Keras Sequential API\n",
    "for i in range(len(epochs_list)):\n",
    "    for j in range(len(batch_size_list)):\n",
    "        for k in range(len(conv_list)):\n",
    "            print(\"##########################################################################\")\n",
    "            print(\"开始训练模型，epochs为\"+str(epochs_list[i])+\", batch_size为\"+str(batch_size_list[j])+\", Conv2D为\"+str(conv_list[k]))\n",
    "            model = models.Sequential()\n",
    "\n",
    "            model.add(layers.Conv2D(conv_list[k][0], (3,3), activation='relu', input_shape=(28,28,1) ))\n",
    "            model.add(layers.Conv2D(conv_list[k][1], (3,3), activation='relu'))\n",
    "            model.add(layers.Conv2D(conv_list[k][2], (3,3), activation='relu'))\n",
    "            model.add(max_pool_1)\n",
    "            model.add(flat_layer)\n",
    "            model.add(fc)\n",
    "            model.add(output)\n",
    "\n",
    "            model.summary()\n",
    "            model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            ################################################\n",
    "            model.fit(train_images_norm, train_labels, epochs=epochs_list[i], batch_size=batch_size_list[j], shuffle=True, validation_split=0.1)\n",
    "            ################################################\n",
    "            test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            new_model = models.Sequential()\n",
    "\n",
    "            new_model.add(conv1_new_conv1)\n",
    "            #第一卷积层'conv1'没有池化层和Dropout层\n",
    "            new_model.add(conv1_new_conv2)\n",
    "            new_model.add(conv1_new_max_pool_2)\n",
    "            new_model.add(drop_2)\n",
    "            new_model.add(conv1_new_conv3)\n",
    "            new_model.add(conv1_new_max_pool_3)\n",
    "            new_model.add(drop_3)\n",
    "            new_model.add(conv1_new_flat_layer)\n",
    "            new_model.add(conv1_new_fc)\n",
    "            new_model.add(BN)\n",
    "            new_model.add(drop_4)\n",
    "            new_model.add(conv1_new_output)\n",
    "\n",
    "            new_model.summary()\n",
    "            new_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            new_model.fit(train_images_norm, train_labels, epochs=30, batch_size=512, shuffle=True, validation_split=0.1)\n",
    "            test_loss, test_accuracy = new_model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            print(test_loss)\n",
    "            print(new_model.history.history.keys())\n",
    "            \n",
    "            #plt.subplot(3,3,3*k+1)\n",
    "            print(\"k=\"+str(k))\n",
    "            plt.figure(3*k+1)\n",
    "            plt.plot(new_model.history.history['accuracy'])\n",
    "            plt.plot(new_model.history.history['val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            #plt.subplot(3,3,3*k+2)\n",
    "            plt.figure(3*k+2)\n",
    "            plt.plot(new_model.history.history['loss'])\n",
    "            plt.plot(new_model.history.history['val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            ############模型预测#################\n",
    "#             for key in range(5)\n",
    "#             predicted_test_labels = new_model.predict(test_images_norm)\n",
    "#             print(predicted_test_labels.shape)\n",
    "#             print(predicted_test_labels[88])\n",
    "\n",
    "\n",
    "#             predicted_test_labels_index = np.argmax(predicted_test_labels[88])\n",
    "\n",
    "#             print(predicted_test_labels_index)\n",
    "#             print(test_labels[88])\n",
    "\n",
    "            #plt.subplot(3,3,3*k+3)\n",
    "            # plt.figure(3*k+3)\n",
    "            # plt.figure()\n",
    "            # plt.imshow(np.squeeze(test_images[88]))\n",
    "            # plt.show()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for key in range(5):\n",
    "                number = random.randint(0,10000)\n",
    "                print(\"number is \"+str(number))\n",
    "                plt.subplot(1,5,key+1)\n",
    "                predicted_test_labels = new_model.predict(test_images_norm)\n",
    "                print(predicted_test_labels.shape)\n",
    "                print(predicted_test_labels[number])\n",
    "\n",
    "\n",
    "                predicted_test_labels_index = np.argmax(predicted_test_labels[number])\n",
    "\n",
    "                print(predicted_test_labels_index)\n",
    "                print(test_labels[number])\n",
    "\n",
    "                #plt.subplot(3,3,3*k+3)\n",
    "                #plt.figure(3*k+3)\n",
    "                #plt.figure()\n",
    "                plt.imshow(np.squeeze(test_images[number]))\n",
    "            plt.show()\n",
    "print(\"模型不同参数配置成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20064f45-5806-4ab1-ab5a-6737feb1d93f",
   "metadata": {},
   "source": [
    "对正则化参数进行调试，前边模型参数仍然采用实验手册上参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72b18f-c109-4d75-b94c-a2500d4caee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##################依赖库导入######################\n",
    "#################################################\n",
    "import os\n",
    "import struct\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"依赖库导入成功\")\n",
    "#################################################\n",
    "##################参数配置########################\n",
    "#################################################\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "epochs_list = [5,10,20,50]\n",
    "batch_size_list = [64,128,256,512]\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0\n",
    "conv_list =  [[32,64,128]]\n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "max_pool_2 = layers.MaxPooling2D((4,4))\n",
    "max_pool_3 = layers.MaxPooling2D((8,8))\n",
    "max_pool_list = [max_pool_1, max_pool_3, max_pool_3]\n",
    "flat_layer = layers.Flatten()\n",
    "fc = layers.Dense(128, activation='relu')\n",
    "output = layers.Dense(10, 'softmax')\n",
    "\n",
    "drop_1 = keras.layers.Dropout(0.2)\n",
    "drop_2 = keras.layers.Dropout(0.2)\n",
    "drop_3 = keras.layers.Dropout(0.2)\n",
    "drop_4 = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv1_new = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02),input_shape=(28,28,1) )\n",
    "BN=keras.layers.BatchNormalization()\n",
    "conv1_new_conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv1_new_conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv1_new_conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "conv1_new_max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_2 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_3 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_flat_layer = layers.Flatten()\n",
    "conv1_new_fc = layers.Dense(128, activation='relu')\n",
    "conv1_new_output = layers.Dense(10, 'softmax')\n",
    "\n",
    "print(\"参数配置成功\")\n",
    "#################################################\n",
    "##################训练开始########################\n",
    "#################################################\n",
    "# TensorFlow Keras 使用Keras Sequential API\n",
    "for i in range(len(epochs_list)):\n",
    "    for j in range(len(batch_size_list)):\n",
    "        for k in range(len(conv_list)):\n",
    "            print(\"##########################################################################\")\n",
    "            print(\"正则化模型，epochs为\"+str(epochs_list[i])+\", batch_size为\"+str(batch_size_list[j])+\", Conv2D为\"+str(conv_list[k]))\n",
    "            model = models.Sequential()\n",
    "\n",
    "            model.add(conv1)\n",
    "            model.add(conv2)\n",
    "            model.add(conv3)\n",
    "            model.add(max_pool_1)\n",
    "            model.add(flat_layer)\n",
    "            model.add(fc)\n",
    "            model.add(output)\n",
    "\n",
    "            model.summary()\n",
    "            model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            ################################################\n",
    "            model.fit(train_images_norm, train_labels, epochs=20, batch_size=512, shuffle=True, validation_split=0.1)\n",
    "            ################################################\n",
    "            test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            new_model = models.Sequential()\n",
    "\n",
    "            new_model.add(layers.Conv2D(conv_list[k][0], (3,3), activation='relu', input_shape=(28,28,1) ))\n",
    "            #第一卷积层'conv1'没有池化层和Dropout层\n",
    "            new_model.add(layers.Conv2D(conv_list[k][1], (3,3), activation='relu'))\n",
    "            new_model.add(conv1_new_max_pool_2)\n",
    "            new_model.add(drop_2)\n",
    "            new_model.add(layers.Conv2D(conv_list[k][2], (3,3), activation='relu'))\n",
    "            new_model.add(conv1_new_max_pool_3)\n",
    "            new_model.add(drop_3)\n",
    "            new_model.add(conv1_new_flat_layer)\n",
    "            new_model.add(conv1_new_fc)\n",
    "            new_model.add(BN)\n",
    "            new_model.add(drop_4)\n",
    "            new_model.add(conv1_new_output)\n",
    "\n",
    "            new_model.summary()\n",
    "            new_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            new_model.fit(train_images_norm, train_labels, epochs=epochs_list[i], batch_size=batch_size_list[j], shuffle=True, validation_split=0.1)\n",
    "            test_loss, test_accuracy = new_model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            print(test_loss)\n",
    "            print(new_model.history.history.keys())\n",
    "            \n",
    "            #plt.subplot(3,3,3*k+1)\n",
    "            print(\"k=\"+str(k))\n",
    "            plt.figure(3*k+1)\n",
    "            plt.plot(new_model.history.history['accuracy'])\n",
    "            plt.plot(new_model.history.history['val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            #plt.subplot(3,3,3*k+2)\n",
    "            plt.figure(3*k+2)\n",
    "            plt.plot(new_model.history.history['loss'])\n",
    "            plt.plot(new_model.history.history['val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            ############模型预测#################\n",
    "#             for key in range(5)\n",
    "#             predicted_test_labels = new_model.predict(test_images_norm)\n",
    "#             print(predicted_test_labels.shape)\n",
    "#             print(predicted_test_labels[88])\n",
    "\n",
    "\n",
    "#             predicted_test_labels_index = np.argmax(predicted_test_labels[88])\n",
    "\n",
    "#             print(predicted_test_labels_index)\n",
    "#             print(test_labels[88])\n",
    "\n",
    "            #plt.subplot(3,3,3*k+3)\n",
    "            # plt.figure(3*k+3)\n",
    "            # plt.figure()\n",
    "            # plt.imshow(np.squeeze(test_images[88]))\n",
    "            # plt.show()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for key in range(5):\n",
    "                number = random.randint(0,10000)\n",
    "                print(\"number is \"+str(number))\n",
    "                plt.subplot(1,5,key+1)\n",
    "                predicted_test_labels = new_model.predict(test_images_norm)\n",
    "                print(predicted_test_labels.shape)\n",
    "                print(predicted_test_labels[number])\n",
    "\n",
    "\n",
    "                predicted_test_labels_index = np.argmax(predicted_test_labels[number])\n",
    "\n",
    "                print(predicted_test_labels_index)\n",
    "                print(test_labels[number])\n",
    "\n",
    "                #plt.subplot(3,3,3*k+3)\n",
    "                #plt.figure(3*k+3)\n",
    "                #plt.figure()\n",
    "                plt.imshow(np.squeeze(test_images[number]))\n",
    "            plt.show()\n",
    "print(\"模型不同参数配置成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190bbf46-dd3a-4b30-a79c-e46a0a13b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "##################依赖库导入######################\n",
    "#################################################\n",
    "import os\n",
    "import struct\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"依赖库导入成功\")\n",
    "#################################################\n",
    "##################参数配置########################\n",
    "#################################################\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "epochs_list = [20]\n",
    "batch_size_list = [256]\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "train_images_norm = train_images / 255.0\n",
    "test_images_norm = test_images / 255.0\n",
    "conv_list =  [[32,64,128],[64,64,128],[32,32,128]]\n",
    "conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "max_pool_2 = layers.MaxPooling2D((4,4))\n",
    "max_pool_3 = layers.MaxPooling2D((8,8))\n",
    "max_pool_list = [max_pool_1, max_pool_3, max_pool_3]\n",
    "flat_layer = layers.Flatten()\n",
    "fc = layers.Dense(128, activation='relu')\n",
    "output = layers.Dense(10, 'softmax')\n",
    "\n",
    "drop_1 = keras.layers.Dropout(0.2)\n",
    "drop_2 = keras.layers.Dropout(0.2)\n",
    "drop_3 = keras.layers.Dropout(0.2)\n",
    "drop_4 = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv1_new = layers.Conv2D(32, (3,3), activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02),input_shape=(28,28,1) )\n",
    "BN=keras.layers.BatchNormalization()\n",
    "conv1_new_conv1 = layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1) )\n",
    "conv1_new_conv2 = layers.Conv2D(64, (3,3), activation='relu')\n",
    "conv1_new_conv3 = layers.Conv2D(128, (3,3), activation='relu')\n",
    "conv1_new_max_pool_1 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_2 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_max_pool_3 = layers.MaxPooling2D((2,2))\n",
    "conv1_new_flat_layer = layers.Flatten()\n",
    "conv1_new_fc = layers.Dense(128, activation='relu')\n",
    "conv1_new_output = layers.Dense(10, 'softmax')\n",
    "\n",
    "print(\"参数配置成功\")\n",
    "#################################################\n",
    "##################训练开始########################\n",
    "#################################################\n",
    "# TensorFlow Keras 使用Keras Sequential API\n",
    "for i in range(len(epochs_list)):\n",
    "    for j in range(len(batch_size_list)):\n",
    "        for k in range(len(conv_list)):\n",
    "            print(\"##########################################################################\")\n",
    "            print(\"正则化模型，epochs为\"+str(epochs_list[i])+\", batch_size为\"+str(batch_size_list[j])+\", Conv2D为\"+str(conv_list[k]))\n",
    "            model = models.Sequential()\n",
    "\n",
    "            model.add(conv1)\n",
    "            model.add(conv2)\n",
    "            model.add(conv3)\n",
    "            model.add(max_pool_1)\n",
    "            model.add(flat_layer)\n",
    "            model.add(fc)\n",
    "            model.add(output)\n",
    "\n",
    "            model.summary()\n",
    "            model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            ################################################\n",
    "            model.fit(train_images_norm, train_labels, epochs=20, batch_size=512, shuffle=True, validation_split=0.1)\n",
    "            ################################################\n",
    "            test_loss, test_accuracy = model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            new_model = models.Sequential()\n",
    "\n",
    "            new_model.add(layers.Conv2D(conv_list[k][0], (3,3), activation='relu', input_shape=(28,28,1) ))\n",
    "            #第一卷积层'conv1'没有池化层和Dropout层\n",
    "            new_model.add(layers.Conv2D(conv_list[k][1], (3,3), activation='relu'))\n",
    "            new_model.add(conv1_new_max_pool_2)\n",
    "            new_model.add(drop_2)\n",
    "            new_model.add(layers.Conv2D(conv_list[k][2], (3,3), activation='relu'))\n",
    "            new_model.add(conv1_new_max_pool_3)\n",
    "            new_model.add(drop_3)\n",
    "            new_model.add(conv1_new_flat_layer)\n",
    "            new_model.add(conv1_new_fc)\n",
    "            new_model.add(BN)\n",
    "            new_model.add(drop_4)\n",
    "            new_model.add(conv1_new_output)\n",
    "\n",
    "            new_model.summary()\n",
    "            new_model.compile(optimizer='adam',\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "            new_model.fit(train_images_norm, train_labels, epochs=epochs_list[i], batch_size=batch_size_list[j], shuffle=True, validation_split=0.1)\n",
    "            test_loss, test_accuracy = new_model.evaluate(test_images_norm, test_labels)\n",
    "            print(test_accuracy)\n",
    "            print(test_loss)\n",
    "            print(new_model.history.history.keys())\n",
    "            \n",
    "            #plt.subplot(3,3,3*k+1)\n",
    "            print(\"k=\"+str(k))\n",
    "            plt.figure(3*k+1)\n",
    "            plt.plot(new_model.history.history['accuracy'])\n",
    "            plt.plot(new_model.history.history['val_accuracy'])\n",
    "            plt.title(\"Accuracy\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            #plt.subplot(3,3,3*k+2)\n",
    "            plt.figure(3*k+2)\n",
    "            plt.plot(new_model.history.history['loss'])\n",
    "            plt.plot(new_model.history.history['val_loss'])\n",
    "            plt.title(\"Loss\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend(['train', 'test'], loc='upper left')\n",
    "            plt.show()\n",
    "            ############模型预测#################\n",
    "#             for key in range(5)\n",
    "#             predicted_test_labels = new_model.predict(test_images_norm)\n",
    "#             print(predicted_test_labels.shape)\n",
    "#             print(predicted_test_labels[88])\n",
    "\n",
    "\n",
    "#             predicted_test_labels_index = np.argmax(predicted_test_labels[88])\n",
    "\n",
    "#             print(predicted_test_labels_index)\n",
    "#             print(test_labels[88])\n",
    "\n",
    "            #plt.subplot(3,3,3*k+3)\n",
    "            # plt.figure(3*k+3)\n",
    "            # plt.figure()\n",
    "            # plt.imshow(np.squeeze(test_images[88]))\n",
    "            # plt.show()\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for key in range(5):\n",
    "                number = random.randint(0,10000)\n",
    "                print(\"number is \"+str(number))\n",
    "                plt.subplot(1,5,key+1)\n",
    "                predicted_test_labels = new_model.predict(test_images_norm)\n",
    "                print(predicted_test_labels.shape)\n",
    "                print(predicted_test_labels[number])\n",
    "\n",
    "\n",
    "                predicted_test_labels_index = np.argmax(predicted_test_labels[number])\n",
    "\n",
    "                print(predicted_test_labels_index)\n",
    "                print(test_labels[number])\n",
    "\n",
    "                #plt.subplot(3,3,3*k+3)\n",
    "                #plt.figure(3*k+3)\n",
    "                #plt.figure()\n",
    "                plt.imshow(np.squeeze(test_images[number]))\n",
    "            plt.show()\n",
    "print(\"模型不同参数配置成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff32ea7e-1f5a-4181-9d48-59b29259721f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 ('MindSpore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "44c171b9371df8ed367c0b59bc9e738de00bd4a8743e88fe023959761cf0e529"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
